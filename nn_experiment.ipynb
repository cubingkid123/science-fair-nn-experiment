{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["Import prerequisites"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import tensorflow as tf\n", "from openpyxl import load_workbook"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Load in data - MNIST dataset"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["mnist = tf.keras.datasets.mnist\n", "(x_train, y_train), (x_test, y_test) = mnist.load_data()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Normalize data"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["x_train = tf.keras.utils.normalize(x_train, axis=1)\n", "x_test = tf.keras.utils.normalize(x_test, axis=1)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Keep track of tests while running"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["test_number = 1"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Create neural network function"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def create_NN(name, hidden_layers, neurons_per_hl):\n", "    # Define model and input layer\n", "    # Uses Sequential model because of its relative simplicity\n", "    model = tf.keras.models.Sequential()\n", "    model.add(tf.keras.layers.Flatten(input_shape=(28, 28)))\n", "    \n", "    # Create hidden layers\n", "    while hidden_layers > 0:\n", "        # relu activation over sigmoid \n", "        model.add(tf.keras.layers.Dense(neurons_per_hl, activation='relu'))\n", "        hidden_layers -= 1\n\n", "    # Add output layer\n", "    # Softmax due to classification problem\n", "    model.add(tf.keras.layers.Dense(10, activation='softmax'))\n\n", "    # Compile and train model\n", "    # Original experiment used Adam optimizer, now using SGD because the data is simple and I want to keep the models as simple as possible\n", "    # Using sparse categorical crossentropy because of classification problem\n", "    model.compile(optimizer='sgd', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n", "    model.fit(x_train, y_train, epochs=2)\n\n", "    # Save metrics\n", "    test_loss, test_accuracy = model.evaluate(x_test, y_test)\n", "    \n", "    # Display metrics\n", "    print(\"Loss: \" + test_loss)\n", "    print(\"Accuracy: \" + test_accuracy)\n", "    \n", "    # Load in Excel file\n", "    wb = load_workbook('results_v2.xlsx')\n", "    file = wb.active\n\n", "    # Add data to Excel file and save\n", "    file.append([name, test_accuracy, test_loss])\n", "    wb.save(filename='results_v2.xlsx')\n", "    \n", "    # Display test number to estimate time remaining\n", "    print(\"Test number: \" + test_number)\n", "    test_number += 1"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Function calls on create_nn but for each number of hidden layers"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def generator(name, neurons_per_hl):\n", "    hidden_layers = 1\n", "    while hidden_layers <= 5:\n", "        name_and_number = name + \" with \" + str(hidden_layers) + \" layers\"\n", "        create_NN(name_and_number, hidden_layers, neurons_per_hl)\n", "        hidden_layers += 1"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Call on create_nn for no hidden layers since no need to test 5 times"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["create_NN('No Hidden Layer', 0, 0)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Call on generator for each desired test<br>\n", "Note: Very inefficient way of doing this - like this because of the large amount of editing throughout the experimentation design process"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["generator('One Nueron Per Layer', 1)\n", "generator('2 Neurons per Layer', 2)\n", "generator('3 Neurons per Layer', 3)\n", "generator('4 Neurons per Layer', 4)\n", "generator('5 Neurons per Layer', 5)\n", "generator('10 Neurons Per Layer', 10)\n", "generator('20 Neurons Per Layer', 20)\n", "generator('30 Neurons Per Layer', 30)\n", "generator('40 Neurons Per Layer', 40)\n", "generator('50 Neurons Per Layer', 50)\n", "generator('60 Neurons Per Layer', 60)\n", "generator('70 Neurons Per Layer', 70)\n", "generator('80 Neurons Per Layer', 80)\n", "generator('90 Neurons Per Layer', 90)\n", "generator('100 Neurons', 100)\n", "generator('200 Neurons', 200)\n", "generator('300 Neurons', 300)\n", "generator('400 Neurons', 400) # hypothesis - with two layers\n", "generator('500 Neurons', 500)\n", "generator('1000 Neurons', 1000)\n", "generator('5000 Neurons', 5000)\n", "generator('10k Neurons', 10000)"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}